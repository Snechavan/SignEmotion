{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAuyZ3i0/pHcp7HWsX5Y+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Snechavan/SignEmotion/blob/main/Untitled24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clfRca5haa31"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install opencv-python numpy matplotlib scikit-learn\n",
        "\n",
        "# Import required libraries\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Upload the signature image\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the name of the uploaded file\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Read and display the image (in RGB for visualization in Colab)\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Step 2: Preprocess the signature image\n",
        "def preprocess_signature(image_path):\n",
        "    # Read the image in grayscale\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Resize to fixed dimensions for consistency\n",
        "    image = cv2.resize(image, (200, 100))  # Resize to 200x100\n",
        "    # Normalize the image (pixel values between 0 and 1)\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "preprocessed_image = preprocess_signature(image_path)\n",
        "\n",
        "# Display the preprocessed image\n",
        "plt.imshow(preprocessed_image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Extract features from the signature (length, direction)\n",
        "def extract_signature_features(image):\n",
        "    # Perform edge detection (Canny)\n",
        "    edges = cv2.Canny((image * 255).astype(np.uint8), 100, 200)\n",
        "\n",
        "    # Find contours in the edge-detected image (representing strokes)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    # For each contour (stroke), compute length and direction\n",
        "    for contour in contours:\n",
        "        if len(contour) > 1:\n",
        "            # Calculate stroke length (Euclidean distance between first and last points)\n",
        "            length = np.linalg.norm(contour[-1] - contour[0])\n",
        "            features.append(length)\n",
        "\n",
        "            # Calculate stroke direction (angle of the stroke)\n",
        "            dx = contour[-1][0][0] - contour[0][0][0]\n",
        "            dy = contour[-1][0][1] - contour[0][0][1]\n",
        "            direction = np.arctan2(dy, dx)  # Angle of the stroke\n",
        "            features.append(direction)\n",
        "\n",
        "    return features\n",
        "\n",
        "signature_features = extract_signature_features(preprocessed_image)\n",
        "\n",
        "# Display the extracted features\n",
        "print(\"Extracted Features:\", signature_features)\n",
        "\n",
        "# Step 4: Train a machine learning model to predict emotion\n",
        "# Simulated dataset (features and corresponding emotion labels)\n",
        "X = [\n",
        "    [150, 0.5, 120, -0.2],  # Example features for one signature\n",
        "    [130, -0.4, 110, 0.7],  # Another signature with different features\n",
        "    [200, 1.2, 170, -0.5],  # Example features for another signature\n",
        "]  # List of feature vectors from multiple signatures\n",
        "\n",
        "y = ['happy', 'sad', 'angry']  # Corresponding emotion labels\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Predict emotion for the uploaded signature\n",
        "predicted_emotion = model.predict([signature_features])[0]\n",
        "print(f\"Predicted Emotion for the Signature: {predicted_emotion}\")\n"
      ]
    }
  ]
}